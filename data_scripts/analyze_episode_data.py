#!/usr/bin/env python3
"""
æ¨¡ä»¿å­¦ä¹ æ•°æ®åˆ†æè„šæœ¬
ç”¨äºåˆ†æä¸€ä¸ªepisodeçš„4ä¸ª.pklæ–‡ä»¶ï¼šjaw_rgb_data, jaw_depth_data, topdown_map, other_data
"""

import os
import pickle
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from typing import Dict, Any, Optional

def load_episode_data(base_dir: str, episode_filename: str) -> Dict[str, Any]:
    """
    åŠ è½½ä¸€ä¸ªepisodeçš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
    
    Args:
        base_dir: åŸºç¡€æ•°æ®ç›®å½•è·¯å¾„
        episode_filename: episodeæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ï¼Œå¦‚ "2azQ1b91cZZ_ep000001"
    
    Returns:
        åŒ…å«æ‰€æœ‰æ•°æ®çš„å­—å…¸
    """
    base_path = Path(base_dir)
    data_folders = ['jaw_rgb_data', 'jaw_depth_data', 'topdown_map', 'other_data']
    
    episode_data = {}
    
    for folder in data_folders:
        file_path = base_path / folder / f"{episode_filename}.pkl"
        
        if file_path.exists():
            try:
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                episode_data[folder] = data
                print(f"âœ… æˆåŠŸåŠ è½½: {folder}/{episode_filename}.pkl")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {folder}/{episode_filename}.pkl - {e}")
                episode_data[folder] = None
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            episode_data[folder] = None
    
    return episode_data

def get_data_explanation(name: str, data_type: str) -> str:
    """
    æ ¹æ®falcon_evaluator.pyä¸­çš„å®é™…æ•°æ®ä¿å­˜æ–¹å¼æä¾›å‡†ç¡®çš„è§£é‡Šè¯´æ˜
    
    Args:
        name: æ•°æ®å­—æ®µåç§°
        data_type: æ•°æ®ç±»å‹
    
    Returns:
        æ•°æ®è§£é‡Šå­—ç¬¦ä¸²
    """
    explanations = {
        # === è§‚æµ‹æ•°æ® ===
        'jaw_rgb_data': 'ğŸ¤– æœºæ¢°è‡‚æœ«ç«¯æ‘„åƒå¤´RGBå›¾åƒåºåˆ— - ä»batch["agent_0_articulated_agent_jaw_rgb"]æ”¶é›†ï¼Œè½¬æ¢ä¸ºuint8æ ¼å¼ï¼Œå½¢çŠ¶(T,H,W,C)',
        'jaw_depth_data': 'ğŸ“ æœºæ¢°è‡‚æœ«ç«¯æ‘„åƒå¤´æ·±åº¦å›¾åƒåºåˆ— - ä»batch["agent_0_articulated_agent_jaw_depth"]æ”¶é›†ï¼Œfloat32æ ¼å¼ï¼Œå½¢çŠ¶(T,H,W,1)',
        'topdown_map': 'ğŸ—ºï¸ ä¿¯è§†å›¾åœ°å›¾æ•°æ® - å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«mapã€fog_of_war_maskã€agent_map_coordã€agent_angleç­‰å­—æ®µ',
        'rgb': 'ğŸ¥ RGBå›¾åƒåºåˆ—ï¼Œæ¯å¸§åŒ…å«å½©è‰²è§†è§‰ä¿¡æ¯',
        'depth': 'ğŸ“ æ·±åº¦å›¾åƒåºåˆ—ï¼Œæ¯ä¸ªåƒç´ è¡¨ç¤ºåˆ°ç‰©ä½“çš„è·ç¦»',
        'map': 'ğŸ—ºï¸ ç¯å¢ƒçš„ä¿¯è§†å›¾è¡¨ç¤º - uint8æ•°ç»„ï¼Œå½¢çŠ¶(H,W)ï¼Œæ•°å€¼èŒƒå›´[0-44]ï¼Œè¡¨ç¤ºä¸åŒåœ°å›¾å…ƒç´ ',
        'fog_of_war_mask': 'ğŸŒ«ï¸ æˆ˜äº‰è¿·é›¾é®ç½© - boolæ•°ç»„ï¼Œæ ‡è®°agentå·²æ¢ç´¢å’Œæœªæ¢ç´¢çš„åŒºåŸŸ',
        'agent_map_coord': 'ğŸ“ Agentåœ°å›¾åæ ‡ - float32æ•°ç»„ï¼Œå½¢çŠ¶(2,)ï¼Œè¡¨ç¤ºagentåœ¨topdownåœ°å›¾ä¸­çš„[x,y]ä½ç½®',
        'agent_angle': 'ğŸ§­ Agentæœå‘è§’åº¦ - float32å€¼ï¼Œå•ä½å¼§åº¦ï¼Œè¡¨ç¤ºagentåœ¨åœ°å›¾ä¸­çš„æœå‘',
        
        # === åŠ¨ä½œå’Œæ§åˆ¶æ•°æ® ===
        'other_data': 'ğŸ“Š åŒ…å«åŠ¨ä½œã€å¥–åŠ±ã€çŠ¶æ€ç­‰è®­ç»ƒç›¸å…³çš„æ ¸å¿ƒæ•°æ®',
        'actions': 'ğŸ® æ™ºèƒ½ä½“åŠ¨ä½œåºåˆ— - ä»action_data.env_actionsæ”¶é›†ï¼Œæ¯æ­¥çš„ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œï¼Œè½¬æ¢ä¸ºint64æ ¼å¼',
        'global_actions': 'ğŸŒ è·¨ç¯å¢ƒåŠ¨ä½œçŸ©é˜µ - æ‰€æœ‰å¹¶è¡Œç¯å¢ƒçš„åŠ¨ä½œåºåˆ—ï¼Œå½¢çŠ¶(T,envs,action_dim)ï¼ŒåŒ…å«å½“å‰episodeæ•°æ®',
        
        # === å¥–åŠ±æ•°æ® ===
        'rewards': 'ğŸ† é€æ­¥å¥–åŠ±åºåˆ— - ä»envs.step()è¿”å›çš„rewards_læ”¶é›†ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„å³æ—¶å¥–åŠ±',
        'global_rewards': 'ğŸ† Episodeå¥–åŠ±æ±‡æ€» - æ‰€æœ‰å®Œæˆepisodeçš„æ€»å¥–åŠ±åˆ—è¡¨ï¼Œç”¨äºè·¨episodeæ€§èƒ½å¯¹æ¯”',
        
        # === æ©ç æ•°æ® ===
        'masks': 'ğŸ­ Episodeç»§ç»­æ©ç  - åŸºäºdonesè®¡ç®—ï¼ŒTrue=episodeç»§ç»­ï¼ŒFalse=episodeç»“æŸ',
        'global_masks': 'ğŸ­ è·¨ç¯å¢ƒæ©ç æ•°æ® - æ‰€æœ‰å¹¶è¡Œç¯å¢ƒçš„episodeç»“æŸæ ‡å¿—ï¼Œç”¨äºè®­ç»ƒæ—¶åºåˆ—å¤„ç†',
        
        # === Infoæ•°æ®å­—æ®µï¼ˆä»infos[i]æ”¶é›†ï¼‰ ===
        'info_data': 'ğŸ“Š Episodeæ‰§è¡Œä¿¡æ¯å­—å…¸ - ä»envs.step()è¿”å›çš„infosæ”¶é›†ï¼ŒåŒ…å«ä»»åŠ¡ç›¸å…³çš„è¯¦ç»†ä¿¡æ¯',
        'distance_to_goal': 'ğŸ“ åˆ°ç›®æ ‡è·ç¦» - æ™ºèƒ½ä½“å½“å‰ä½ç½®åˆ°ç›®æ ‡çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆç±³ï¼‰',
        'distance_to_goal_reward': 'ğŸ“ è·ç¦»æ”¹å–„å¥–åŠ± - è·ç¦»å˜åŒ–é‡(ç±³)ï¼Œæ­£å€¼=é è¿‘ç›®æ ‡ï¼Œè´Ÿå€¼=è¿œç¦»ç›®æ ‡ï¼Œå…¬å¼:-(å½“å‰è·ç¦»-ä¸Šæ­¥è·ç¦»)',
        'success': 'âœ… ä»»åŠ¡æˆåŠŸæ ‡å¿— - Booleanå€¼ï¼ŒTrueè¡¨ç¤ºæˆåŠŸåˆ°è¾¾ç›®æ ‡æˆ–å®Œæˆä»»åŠ¡',
        'spl': 'ğŸ“ˆ SPLæŒ‡æ ‡ - Success weighted by Path Lengthï¼ŒæˆåŠŸç‡Ã—æœ€çŸ­è·¯å¾„é•¿åº¦/å®é™…è·¯å¾„é•¿åº¦',
        'softspl': 'ğŸ“Š è½¯SPLæŒ‡æ ‡ - è€ƒè™‘éƒ¨åˆ†æˆåŠŸçš„SPLå˜ä½“',
        'num_steps': 'ğŸ‘£ Episodeæ­¥æ•° - å½“å‰episodeå·²æ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°',
        'collisions': 'ğŸ’¥ ç¢°æ’ç»Ÿè®¡ - è®°å½•æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„ç¢°æ’æ¬¡æ•°å’Œç±»å‹',
        'did_multi_agents_collide': 'ğŸ’¥ å¤šæ™ºèƒ½ä½“ç¢°æ’æ£€æµ‹ - æ£€æµ‹ä¸¤ä¸ªæ™ºèƒ½ä½“æ˜¯å¦ç¢°æ’ï¼Œ0.0=æ— ç¢°æ’ï¼Œ1.0=å‘ç”Ÿç¢°æ’',
        'composite_reward': 'ğŸ ç»¼åˆå¥–åŠ±å€¼ï¼Œç»“åˆå¤šä¸ªå¥–åŠ±ç»„ä»¶',
        'force_terminate': 'ğŸ›‘ æ˜¯å¦å¼ºåˆ¶ç»ˆæ­¢episodeçš„æ ‡å¿—',
        
        # === Episodeå’Œåœºæ™¯ä¿¡æ¯ ===
        'episode_stats': 'ğŸ“ˆ Episodeç»Ÿè®¡æ‘˜è¦ - åŒ…å«rewardå’Œextract_scalars_from_info()æå–çš„æ‰€æœ‰æ ‡é‡æŒ‡æ ‡',
        'running_episode_stats': 'ğŸ“Š è¿è¡Œæ—¶ç»Ÿè®¡ä¿¡æ¯ - ä»eval_data_collectionæ”¶é›†ï¼ŒåŒ…å«è½¨è¿¹æ•°æ®å’Œè®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¿¡æ¯ï¼Œå¯é€‰ä¿å­˜',
        'scene_id': 'ğŸ  3Dåœºæ™¯æ ‡è¯† - æŒ‡å®šepisodeä½¿ç”¨çš„Habitatåœºæ™¯æ–‡ä»¶è·¯å¾„',
        'episode_id': 'ğŸ¬ Episodeå”¯ä¸€ID - åœ¨æ•°æ®é›†ä¸­å”¯ä¸€æ ‡è¯†ä¸€ä¸ªepisodeå®ä¾‹',
        
        # === GPSå’Œå¯¼èˆªæ•°æ® ===
        'agent_0_pointgoal_with_gps_compass': 'ğŸ§­ æ™ºèƒ½ä½“GPSå¯¼èˆª - ä»batchæ”¶é›†ï¼ŒåŒ…å«ç›¸å¯¹ç›®æ ‡çš„GPSåæ ‡(x,y)ï¼Œfloat32æ ¼å¼',
        'pointgoal_with_gps_compass': 'ğŸ§­ ç›®æ ‡ç‚¹GPSä¿¡æ¯ - æ™ºèƒ½ä½“ç›¸å¯¹äºç›®æ ‡ç‚¹çš„GPSåæ ‡å’Œæ–¹å‘ä¿¡æ¯',
        
        # === æ£€æŸ¥ç‚¹å’Œè®­ç»ƒç›¸å…³ ===
        'checkpoint_index': 'ğŸ”¢ æ¨¡å‹æ£€æŸ¥ç‚¹ç´¢å¼• - å¯¹åº”è®­ç»ƒçš„updateæ¬¡æ•°ï¼Œç”¨äºæ ‡è¯†æ¨¡å‹ç‰ˆæœ¬å’Œæ•°æ®ç‰ˆæœ¬',
        'update_count': 'ğŸ”„ æ›´æ–°è®¡æ•° - ç­‰åŒäºcheckpoint_indexï¼Œè¡¨ç¤ºè®­ç»ƒæ›´æ–°æ¬¡æ•°',
        'num_envs': 'ğŸŒ ç¯å¢ƒæ•°é‡ - å¹¶è¡Œè¿è¡Œçš„ç¯å¢ƒå®ä¾‹æ•°é‡',
        'total_steps': 'ğŸ“ æ€»æ­¥æ•° - æ‰€æœ‰ç¯å¢ƒç´¯è®¡æ‰§è¡Œçš„æ­¥æ•°',
        'trajectory': 'ğŸ›¤ï¸ è½¨è¿¹æ•°æ® - åŒ…å«æ¯æ­¥çš„actionã€positionã€headingç­‰ä¿¡æ¯ï¼Œç”¨äºè·¯å¾„åˆ†æ'
    }
    
    # å°è¯•ç²¾ç¡®åŒ¹é…
    if name in explanations:
        return explanations[name]
    
    # å°è¯•éƒ¨åˆ†åŒ¹é…
    for key, explanation in explanations.items():
        if key in name.lower() or name.lower() in key:
            return explanation
    
    # æ ¹æ®æ•°æ®ç±»å‹æä¾›é€šç”¨è§£é‡Š
    if data_type == 'numpy.ndarray':
        return 'ğŸ”¢ æ•°å€¼æ•°ç»„æ•°æ®'
    elif data_type == 'dict':
        return 'ğŸ“ å­—å…¸ç»“æ„æ•°æ®'
    elif data_type in ['list', 'tuple']:
        return 'ğŸ“‹ åºåˆ—æ•°æ®'
    else:
        return 'ğŸ“„ å…¶ä»–ç±»å‹æ•°æ®'

def analyze_data_structure(data: Any, name: str, indent: int = 0) -> None:
    """
    åˆ†ææ•°æ®ç»“æ„å¹¶æä¾›è§£é‡Šè¯´æ˜
    
    Args:
        data: è¦åˆ†æçš„æ•°æ®
        name: æ•°æ®åç§°
        indent: ç¼©è¿›çº§åˆ«
    """
    prefix = "  " * indent
    
    if isinstance(data, dict):
        explanation = get_data_explanation(name, 'dict')
        print(f"{prefix}ğŸ“ {name}: dict (åŒ…å« {len(data)} ä¸ªé”®)")
        print(f"{prefix}   ğŸ’¡ è¯´æ˜: {explanation}")
        for key, value in data.items():
            analyze_data_structure(value, key, indent + 1)
    
    elif isinstance(data, (list, tuple)):
        explanation = get_data_explanation(name, type(data).__name__)
        print(f"{prefix}ğŸ“‹ {name}: {type(data).__name__} (é•¿åº¦: {len(data)})")
        print(f"{prefix}   ğŸ’¡ è¯´æ˜: {explanation}")
        if len(data) > 0:
            print(f"{prefix}   ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(data[0])}")
            if hasattr(data[0], 'shape'):
                print(f"{prefix}   ç¬¬ä¸€ä¸ªå…ƒç´ å½¢çŠ¶: {data[0].shape}")
    
    elif isinstance(data, np.ndarray):
        explanation = get_data_explanation(name, 'numpy.ndarray')
        print(f"{prefix}ğŸ”¢ {name}: numpy.ndarray")
        print(f"{prefix}   ğŸ’¡ è¯´æ˜: {explanation}")
        print(f"{prefix}   å½¢çŠ¶: {data.shape}")
        print(f"{prefix}   æ•°æ®ç±»å‹: {data.dtype}")
        if data.size > 0:
            print(f"{prefix}   æ•°å€¼èŒƒå›´: [{np.min(data):.3f}, {np.max(data):.3f}]")
            if len(data.shape) > 0 and data.shape[0] > 0:
                print(f"{prefix}   ç¬¬ä¸€ä¸ªå…ƒç´ : {data[0] if data.ndim == 1 else 'shape=' + str(data[0].shape)}")
    
    else:
        explanation = get_data_explanation(name, type(data).__name__)
        print(f"{prefix}ğŸ“„ {name}: {type(data).__name__} = {data}")
        print(f"{prefix}   ğŸ’¡ è¯´æ˜: {explanation}")

def analyze_episode_summary(other_data: Dict[str, Any]) -> None:
    """
    åˆ†æepisodeçš„æ€»ç»“ä¿¡æ¯
    
    Args:
        other_data: other_data.pklä¸­çš„æ•°æ®
    """
    print("\n" + "="*60)
    print("ğŸ“Š EPISODE æ€»ç»“ä¿¡æ¯")
    print("="*60)
    
    if 'episode_stats' in other_data:
        stats = other_data['episode_stats']
        print("ğŸ† Episodeç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
    
    if 'info_data' in other_data and other_data['info_data'] is not None:
        info_data = other_data['info_data']
        
        if isinstance(info_data, dict):
            # æ–°æ ¼å¼ï¼šinfo_dataæ˜¯å­—å…¸ï¼Œæ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªnumpyæ•°ç»„
            print("\nğŸ“‹ info_dataç»“æ„ (numpyæ•°ç»„æ ¼å¼):")
            for key, value in info_data.items():
                if isinstance(value, np.ndarray):
                    print(f"   {key}: shape={value.shape}, dtype={value.dtype}")
                    if len(value) > 0:
                        print(f"      æœ€åä¸€å¸§å€¼: {value[-1]}")
                        print(f"      æ•°å€¼èŒƒå›´: [{np.min(value):.3f}, {np.max(value):.3f}]")
                else:
                    print(f"   {key}: {type(value)} = {value}")
        elif isinstance(info_data, list) and len(info_data) > 0:
            # æ—§æ ¼å¼ï¼šinfo_dataæ˜¯å­—å…¸åˆ—è¡¨
            final_info = info_data[-1]  # æœ€åä¸€å¸§çš„info
            print("\nğŸ¯ æœ€åä¸€å¸§ä¿¡æ¯:")
            if isinstance(final_info, dict):
                analyze_data_structure(final_info, "æœ€åä¸€å¸§info", 1)
            else:
                print(f"   ç±»å‹: {type(final_info)}, å€¼: {final_info}")
            
            # æ˜¾ç¤ºinfo_dataçš„æ•´ä½“ç»“æ„
            print("\nğŸ“‹ info_dataæ•´ä½“ç»“æ„:")
            print(f"   æ€»å¸§æ•°: {len(info_data)}")
            if len(info_data) > 0:
                print("   ç¬¬ä¸€å¸§infoç»“æ„:")
                analyze_data_structure(info_data[0], "ç¬¬ä¸€å¸§info", 1)
        else:
            print("\nğŸ“‹ info_dataä¸ºç©ºæˆ–æ ¼å¼æœªçŸ¥")
            print(f"   ç±»å‹: {type(info_data)}")
            if hasattr(info_data, '__len__'):
                print(f"   é•¿åº¦: {len(info_data)}")
    
    # åˆ†æå¥–åŠ±åºåˆ—
    if 'rewards' in other_data:
        rewards = other_data['rewards']
        if isinstance(rewards, np.ndarray) and len(rewards) > 0:
            print(f"\nğŸ’° å¥–åŠ±åˆ†æ:")
            print(f"   æ€»å¥–åŠ±: {np.sum(rewards):.3f}")
            print(f"   å¹³å‡å¥–åŠ±: {np.mean(rewards):.3f}")
            print(f"   æœ€ç»ˆå¥–åŠ±: {rewards[-1]:.3f}")
            print(f"   å¥–åŠ±èŒƒå›´: [{np.min(rewards):.3f}, {np.max(rewards):.3f}]")

# å¯è§†åŒ–å‡½æ•°å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ generate_episode_video.py ç”Ÿæˆè§†é¢‘

def analyze_episode(base_dir: str, episode_filename: str, visualize: bool = True) -> None:
    """
    åˆ†æä¸€ä¸ªepisodeçš„å®Œæ•´æ•°æ®
    
    Args:
        base_dir: åŸºç¡€æ•°æ®ç›®å½•
        episode_filename: episodeæ–‡ä»¶å
        visualize: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–
    """
    print("="*80)
    print(f"ğŸ” åˆ†æEpisode: {episode_filename}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {base_dir}")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    episode_data = load_episode_data(base_dir, episode_filename)
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶çš„æ•°æ®ç»“æ„
    for folder_name, data in episode_data.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“‚ {folder_name.upper()} æ•°æ®ç»“æ„åˆ†æ")
        print(f"{'='*60}")
        
        if data is not None:
            analyze_data_structure(data, folder_name)
        else:
            print("âŒ æ•°æ®ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")
    
    # åˆ†æepisodeæ€»ç»“
    if episode_data.get('other_data') is not None:
        analyze_episode_summary(episode_data['other_data'])
    
    # å¯è§†åŒ–å·²ç§»é™¤ï¼Œå¦‚éœ€ç”Ÿæˆè§†é¢‘è¯·ä½¿ç”¨ generate_episode_video.py
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆ")
    print("="*80)

def find_latest_data_directory() -> str:
    """
    è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨¡ä»¿å­¦ä¹ æ•°æ®ç›®å½•
    
    Returns:
        æœ€æ–°æ•°æ®ç›®å½•çš„å®Œæ•´è·¯å¾„
    """
    falcon_data_root = "/root/zwj/Falcon/falcon_imitation_data"
    
    if not os.path.exists(falcon_data_root):
        raise FileNotFoundError(f"æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {falcon_data_root}")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•ï¼ˆæ ¼å¼ï¼šYYYYMMDD_HHMMSSï¼‰
    timestamp_dirs = []
    for item in os.listdir(falcon_data_root):
        item_path = os.path.join(falcon_data_root, item)
        if os.path.isdir(item_path) and len(item) == 15 and '_' in item:
            try:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¶é—´æˆ³æ ¼å¼
                date_part, time_part = item.split('_')
                if len(date_part) == 8 and len(time_part) == 6:
                    timestamp_dirs.append(item)
            except ValueError:
                continue
    
    if not timestamp_dirs:
        raise FileNotFoundError(f"åœ¨ {falcon_data_root} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ—¶é—´æˆ³ç›®å½•")
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
    latest_timestamp = sorted(timestamp_dirs)[-1]
    latest_dir = os.path.join(falcon_data_root, latest_timestamp)
    
    print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ•°æ®ç›®å½•: {latest_dir}")
    return latest_dir

def find_episodes(base_dir: str) -> list:
    """
    æŸ¥æ‰¾ç›®å½•ä¸­æ‰€æœ‰å¯ç”¨çš„episode
    
    Args:
        base_dir: åŸºç¡€æ•°æ®ç›®å½•
    
    Returns:
        episodeæ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰
    """
    base_path = Path(base_dir)
    episodes = set()
    
    # ä»other_dataæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æ‰€æœ‰episode
    other_data_dir = base_path / 'other_data'
    if other_data_dir.exists():
        for pkl_file in other_data_dir.glob('*.pkl'):
            episode_name = pkl_file.stem  # å»æ‰.pklæ‰©å±•å
            episodes.add(episode_name)
    
    return sorted(list(episodes))

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # å¦‚æœæä¾›äº†episodeåç§°
        episode_name = sys.argv[1]
        if len(sys.argv) > 2:
            # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ç›®å½•
            base_dir = sys.argv[2]
        else:
            # è‡ªåŠ¨æ£€æµ‹æœ€æ–°æ•°æ®ç›®å½•
            try:
                base_dir = find_latest_data_directory()
            except FileNotFoundError as e:
                print(f"âŒ {e}")
                print("è¯·æ‰‹åŠ¨æŒ‡å®šæ•°æ®ç›®å½•è·¯å¾„")
                sys.exit(1)
        analyze_episode(base_dir, episode_name)
    else:
        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„episode
        try:
            base_dir = find_latest_data_directory()
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            print("è¯·æ‰‹åŠ¨æŒ‡å®šæ•°æ®ç›®å½•è·¯å¾„")
            sys.exit(1)
            
        episodes = find_episodes(base_dir)
        
        if episodes:
            print(f"ğŸ“‹ æ‰¾åˆ° {len(episodes)} ä¸ªepisode:")
            for i, episode in enumerate(episodes[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   {i+1}. {episode}")
            
            if len(episodes) > 10:
                print(f"   ... è¿˜æœ‰ {len(episodes) - 10} ä¸ªepisode")
            
            # åˆ†æç¬¬ä¸€ä¸ªepisode
            print(f"\nğŸ¯ è‡ªåŠ¨åˆ†æç¬¬ä¸€ä¸ªepisode: {episodes[0]}")
            analyze_episode(base_dir, episodes[0])
        else:
            print(f"âŒ åœ¨ {base_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•episodeæ•°æ®")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print(f"  python {sys.argv[0]} <episode_name> [base_dir]")
            print(f"  ä¾‹å¦‚: python {sys.argv[0]} 2azQ1b91cZZ_ep000001")